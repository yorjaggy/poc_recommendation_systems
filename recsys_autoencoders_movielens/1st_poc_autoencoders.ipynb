{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "    https://medium.com/@connectwithghosh/recommender-system-on-the-movielens-using-an-autoencoder-using-tensorflow-in-python-f13d3e8d600d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "import tensorflow as tf\n",
    "# Importing some more libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2          3\n",
       "0  1  1193  5  978300760\n",
       "1  1   661  3  978302109"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the ratings data\n",
    "# contains UserID::MovieID::Rating::Timestamp\n",
    "ratings = pd.read_csv('../datasets/ml-1m/ratings.dat',\\\n",
    "          sep=\"::\", header = None, engine='python')\n",
    "\n",
    "ratings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting some features\n",
    "\n",
    "UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "- UserIDs range between 1 and 6040 \n",
    "- MovieIDs range between 1 and 3952\n",
    "- Ratings are made on a 5-star scale (whole-star ratings only)\n",
    "- Timestamp is represented in seconds since the epoch as returned by time(2) (dropped)\n",
    "- Each user has at least 20 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>3943</th>\n",
       "      <th>3944</th>\n",
       "      <th>3945</th>\n",
       "      <th>3946</th>\n",
       "      <th>3947</th>\n",
       "      <th>3948</th>\n",
       "      <th>3949</th>\n",
       "      <th>3950</th>\n",
       "      <th>3951</th>\n",
       "      <th>3952</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 3706 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "1  1     2     3     4     5     6     7     8     9     10    ...  3943  \\\n",
       "0                                                              ...         \n",
       "1   5.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "1  3944  3945  3946  3947  3948  3949  3950  3951  3952  \n",
       "0                                                        \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[2 rows x 3706 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets pivot the data to get it at a user level\n",
    "# To generate a table with userXmovies\n",
    "# 0)UserID::1)MovieID::2)Rating::3)Timestamp\n",
    "ratings_pivot = pd.pivot_table(ratings[[0,1,2]],\\\n",
    "          values=2, index=0, columns=1 ).fillna(0)\n",
    "\n",
    "ratings_pivot.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_pivot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train and test sets\n",
    "X_train, X_test = train_test_split(ratings_pivot, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainset:rows 4832 and cols 3706'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'testset:rows 1208 and cols 3706'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train.shape[0]\n",
    "display(\"trainset:rows {} and cols {}\".format(X_train.shape[0],X_train.shape[1]))\n",
    "display(\"testset:rows {} and cols {}\".format(X_test.shape[0],X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deciding how many nodes wach layer should have\n",
    "n_nodes_inpl = 3706\n",
    "n_nodes_hl1  = 256\n",
    "n_nodes_outl = 3706  \n",
    "\n",
    "hidden_1_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_inpl+1,n_nodes_hl1]))}\n",
    "\n",
    "output_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1+1,n_nodes_outl])) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The bias matrix is not created, due to was added a bias node to each layer which has a contant value = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0916 20:59:56.061133 140424641423168 deprecation.py:506] From /home/i2t/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# user with 3706 ratings goes in\n",
    "input_layer = tf.placeholder('float', [None, 3706])\n",
    "# add a constant node to the first layer\n",
    "# it needs to have the same shape as the input layer for me to be\n",
    "# able to concatinate it later\n",
    "input_layer_const = tf.fill( [tf.shape(input_layer)[0], 1] ,1.0  )\n",
    "input_layer_concat =  tf.concat([input_layer, input_layer_const], 1)\n",
    "# multiply output of input_layer wth a weight matrix \n",
    "layer_1 = tf.nn.sigmoid(tf.matmul(input_layer_concat,hidden_1_layer_vals['weights']))\n",
    "# adding one bias node to the hidden layer\n",
    "layer1_const = tf.fill( [tf.shape(layer_1)[0], 1] ,1.0  )\n",
    "layer_concat =  tf.concat([layer_1, layer1_const], 1)\n",
    "# multiply output of hidden with a weight matrix to get final output\n",
    "output_layer = tf.matmul( layer_concat,output_layer_vals['weights'])\n",
    "# output_true shall have the original shape for error calculations\n",
    "output_true = tf.placeholder('float', [None, 3706])\n",
    "# define our cost function\n",
    "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "# define our optimizer\n",
    "learn_rate = 0.1   # how fast the model should learn\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising variables and starting the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# defining batch size, number of epochs and learning rate\n",
    "batch_size = 100  # how many images to use together for training\n",
    "hm_epochs = 200    # how many times to go through the entire dataset\n",
    "tot_users = X_train.shape[0] # total number of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train 57.39390116577901 MSE test 57.38934683141875\n",
      "Epoch 0 / 200 loss: 3900.6919326782227\n",
      "MSE train 38.74890892495517 MSE test 38.6264581985595\n",
      "Epoch 1 / 200 loss: 2272.209987640381\n",
      "MSE train 28.57256802106744 MSE test 28.33049441343024\n",
      "Epoch 2 / 200 loss: 1606.6325798034668\n",
      "MSE train 22.056296315056844 MSE test 21.801614362521907\n",
      "Epoch 3 / 200 loss: 1212.1367206573486\n",
      "MSE train 17.966431810009347 MSE test 17.72979923083822\n",
      "Epoch 4 / 200 loss: 961.5949630737305\n",
      "MSE train 15.382106282760871 MSE test 15.197460855405028\n",
      "Epoch 5 / 200 loss: 802.8688144683838\n",
      "MSE train 13.533191625399228 MSE test 13.427552588569121\n",
      "Epoch 6 / 200 loss: 697.210147857666\n",
      "MSE train 12.144757350662331 MSE test 12.108740681297617\n",
      "Epoch 7 / 200 loss: 619.8052282333374\n",
      "MSE train 11.069811636424141 MSE test 11.086406935737093\n",
      "Epoch 8 / 200 loss: 560.4802780151367\n",
      "MSE train 10.20652292856114 MSE test 10.26344200669845\n",
      "Epoch 9 / 200 loss: 513.6228876113892\n",
      "MSE train 9.506388863110384 MSE test 9.567601012902006\n",
      "Epoch 10 / 200 loss: 475.87007093429565\n",
      "MSE train 8.927709321731225 MSE test 8.995182644023894\n",
      "Epoch 11 / 200 loss: 444.70418548583984\n",
      "MSE train 8.440939822558086 MSE test 8.521995985783839\n",
      "Epoch 12 / 200 loss: 419.0941071510315\n",
      "MSE train 8.013781868613389 MSE test 8.108427975133006\n",
      "Epoch 13 / 200 loss: 396.8785791397095\n",
      "MSE train 7.6398857709157255 MSE test 7.7484734766415\n",
      "Epoch 14 / 200 loss: 377.5440764427185\n",
      "MSE train 7.301644786633458 MSE test 7.420561253800052\n",
      "Epoch 15 / 200 loss: 360.31396484375\n",
      "MSE train 7.001381202692906 MSE test 7.130994238686788\n",
      "Epoch 16 / 200 loss: 344.8432593345642\n",
      "MSE train 6.736680114356419 MSE test 6.877947998194705\n",
      "Epoch 17 / 200 loss: 331.1263155937195\n",
      "MSE train 6.49648169270041 MSE test 6.647154165626648\n",
      "Epoch 18 / 200 loss: 318.9137406349182\n",
      "MSE train 6.273585761358609 MSE test 6.435831790300635\n",
      "Epoch 19 / 200 loss: 307.70688247680664\n",
      "MSE train 6.074858306538449 MSE test 6.248450446460375\n",
      "Epoch 20 / 200 loss: 297.4306073188782\n",
      "MSE train 5.895260774974148 MSE test 6.0800534040735235\n",
      "Epoch 21 / 200 loss: 288.26538944244385\n",
      "MSE train 5.734755911096998 MSE test 5.929162489938187\n",
      "Epoch 22 / 200 loss: 279.9396026134491\n",
      "MSE train 5.587955552983853 MSE test 5.790144592592837\n",
      "Epoch 23 / 200 loss: 272.5107481479645\n",
      "MSE train 5.449981712050726 MSE test 5.6602996886305865\n",
      "Epoch 24 / 200 loss: 265.6077139377594\n",
      "MSE train 5.320055665094047 MSE test 5.538234666929181\n",
      "Epoch 25 / 200 loss: 259.1366171836853\n",
      "MSE train 5.198809833637054 MSE test 5.42369428592021\n",
      "Epoch 26 / 200 loss: 253.03751420974731\n",
      "MSE train 5.085173444116708 MSE test 5.316346712267122\n",
      "Epoch 27 / 200 loss: 247.34235906600952\n",
      "MSE train 4.978073445576576 MSE test 5.215713828895225\n",
      "Epoch 28 / 200 loss: 241.99119687080383\n",
      "MSE train 4.875719080837453 MSE test 5.120203834775449\n",
      "Epoch 29 / 200 loss: 236.93724513053894\n",
      "MSE train 4.778287322655826 MSE test 5.029525386517629\n",
      "Epoch 30 / 200 loss: 232.10691857337952\n",
      "MSE train 4.686794360377515 MSE test 4.944411675273717\n",
      "Epoch 31 / 200 loss: 227.5201382637024\n",
      "MSE train 4.599902398065525 MSE test 4.864998157198333\n",
      "Epoch 32 / 200 loss: 223.20739459991455\n",
      "MSE train 4.517295271434658 MSE test 4.790642740421383\n",
      "Epoch 33 / 200 loss: 219.10233426094055\n",
      "MSE train 4.439237310005445 MSE test 4.7204479021375665\n",
      "Epoch 34 / 200 loss: 215.21203470230103\n",
      "MSE train 4.364185206865854 MSE test 4.653474249352407\n",
      "Epoch 35 / 200 loss: 211.5190360546112\n",
      "MSE train 4.291719550025437 MSE test 4.589117881880616\n",
      "Epoch 36 / 200 loss: 207.96517372131348\n",
      "MSE train 4.222282701864923 MSE test 4.527590648866318\n",
      "Epoch 37 / 200 loss: 204.5382947921753\n",
      "MSE train 4.15850603079599 MSE test 4.469554628469912\n",
      "Epoch 38 / 200 loss: 201.27450728416443\n",
      "MSE train 4.099435869342281 MSE test 4.414962465584824\n",
      "Epoch 39 / 200 loss: 198.28922319412231\n",
      "MSE train 4.042664836599856 MSE test 4.3628077172135855\n",
      "Epoch 40 / 200 loss: 195.49973940849304\n",
      "MSE train 3.987526675529405 MSE test 4.311890369738971\n",
      "Epoch 41 / 200 loss: 192.80625295639038\n",
      "MSE train 3.9350477419506267 MSE test 4.2629674255553835\n",
      "Epoch 42 / 200 loss: 190.19680070877075\n",
      "MSE train 3.8840804640251756 MSE test 4.215710419077721\n",
      "Epoch 43 / 200 loss: 187.7050564289093\n",
      "MSE train 3.8338472378969235 MSE test 4.169605417084493\n",
      "Epoch 44 / 200 loss: 185.27812123298645\n",
      "MSE train 3.7853152145995557 MSE test 4.124827693679896\n",
      "Epoch 45 / 200 loss: 182.8904185295105\n",
      "MSE train 3.7375676393563007 MSE test 4.081140978003157\n",
      "Epoch 46 / 200 loss: 180.57988381385803\n",
      "MSE train 3.6914257869683165 MSE test 4.038593724332228\n",
      "Epoch 47 / 200 loss: 178.30787682533264\n",
      "MSE train 3.646813096640745 MSE test 3.9980827826910392\n",
      "Epoch 48 / 200 loss: 176.1169548034668\n",
      "MSE train 3.6031358930490742 MSE test 3.9587559215528874\n",
      "Epoch 49 / 200 loss: 173.99263167381287\n",
      "MSE train 3.5612570902045975 MSE test 3.920886960606708\n",
      "Epoch 50 / 200 loss: 171.9215874671936\n",
      "MSE train 3.5206695103091885 MSE test 3.8843421606038415\n",
      "Epoch 51 / 200 loss: 169.9325397014618\n",
      "MSE train 3.4809572602903636 MSE test 3.8490091681294447\n",
      "Epoch 52 / 200 loss: 168.00115370750427\n",
      "MSE train 3.442982602141072 MSE test 3.8150345239967764\n",
      "Epoch 53 / 200 loss: 166.1156120300293\n",
      "MSE train 3.4060438036500416 MSE test 3.782057191722674\n",
      "Epoch 54 / 200 loss: 164.30883646011353\n",
      "MSE train 3.370524271600187 MSE test 3.7500755883332815\n",
      "Epoch 55 / 200 loss: 162.55169820785522\n",
      "MSE train 3.3365787305545997 MSE test 3.7190316687745413\n",
      "Epoch 56 / 200 loss: 160.86607098579407\n",
      "MSE train 3.3035819156525235 MSE test 3.688628905932213\n",
      "Epoch 57 / 200 loss: 159.2515127658844\n",
      "MSE train 3.271438388826802 MSE test 3.658874137257472\n",
      "Epoch 58 / 200 loss: 157.68112301826477\n",
      "MSE train 3.2402477598453436 MSE test 3.6299017640313904\n",
      "Epoch 59 / 200 loss: 156.15103220939636\n",
      "MSE train 3.209756979677312 MSE test 3.601747367890845\n",
      "Epoch 60 / 200 loss: 154.66803336143494\n",
      "MSE train 3.180057924878237 MSE test 3.5744654203609367\n",
      "Epoch 61 / 200 loss: 153.21672463417053\n",
      "MSE train 3.151996157636702 MSE test 3.548530237792627\n",
      "Epoch 62 / 200 loss: 151.80936884880066\n",
      "MSE train 3.124939334165377 MSE test 3.523383646802885\n",
      "Epoch 63 / 200 loss: 150.4749457836151\n",
      "MSE train 3.09835311238077 MSE test 3.4987561592641523\n",
      "Epoch 64 / 200 loss: 149.18465948104858\n",
      "MSE train 3.0722048544006415 MSE test 3.474740650695063\n",
      "Epoch 65 / 200 loss: 147.9179253578186\n",
      "MSE train 3.0466636103648477 MSE test 3.4512332817002136\n",
      "Epoch 66 / 200 loss: 146.67044019699097\n",
      "MSE train 3.0217741410646153 MSE test 3.4282011497909006\n",
      "Epoch 67 / 200 loss: 145.45263481140137\n",
      "MSE train 2.997173474586345 MSE test 3.4055113954322587\n",
      "Epoch 68 / 200 loss: 144.26291251182556\n",
      "MSE train 2.9735871908623057 MSE test 3.3836655302068803\n",
      "Epoch 69 / 200 loss: 143.09464621543884\n",
      "MSE train 2.950619423528433 MSE test 3.3625720101989938\n",
      "Epoch 70 / 200 loss: 141.9719376564026\n",
      "MSE train 2.9278360372035768 MSE test 3.3419035488035687\n",
      "Epoch 71 / 200 loss: 140.87529718875885\n",
      "MSE train 2.906299032322541 MSE test 3.3219726314491482\n",
      "Epoch 72 / 200 loss: 139.79095149040222\n",
      "MSE train 2.88604827295174 MSE test 3.3028419820365573\n",
      "Epoch 73 / 200 loss: 138.7680389881134\n",
      "MSE train 2.8663308372753358 MSE test 3.2843253018021814\n",
      "Epoch 74 / 200 loss: 137.8040462732315\n",
      "MSE train 2.847271517863195 MSE test 3.2662517789005445\n",
      "Epoch 75 / 200 loss: 136.86525559425354\n",
      "MSE train 2.8283540772068623 MSE test 3.248360301275281\n",
      "Epoch 76 / 200 loss: 135.95558774471283\n",
      "MSE train 2.8095284226919723 MSE test 3.230672667274579\n",
      "Epoch 77 / 200 loss: 135.04980719089508\n",
      "MSE train 2.7910920370234384 MSE test 3.2133442359169253\n",
      "Epoch 78 / 200 loss: 134.15238785743713\n",
      "MSE train 2.773042904979725 MSE test 3.1964432533220077\n",
      "Epoch 79 / 200 loss: 133.2732560634613\n",
      "MSE train 2.755132642957316 MSE test 3.1797783739393743\n",
      "Epoch 80 / 200 loss: 132.4111523628235\n",
      "MSE train 2.737473275165921 MSE test 3.163238232832341\n",
      "Epoch 81 / 200 loss: 131.55526447296143\n",
      "MSE train 2.7198595380703803 MSE test 3.146851072708938\n",
      "Epoch 82 / 200 loss: 130.7110368013382\n",
      "MSE train 2.702454681673398 MSE test 3.1305439435930196\n",
      "Epoch 83 / 200 loss: 129.86632168293\n",
      "MSE train 2.6857525052270534 MSE test 3.1146923710772416\n",
      "Epoch 84 / 200 loss: 129.03759098052979\n",
      "MSE train 2.6696387480162165 MSE test 3.0992398027729933\n",
      "Epoch 85 / 200 loss: 128.24146258831024\n",
      "MSE train 2.6534968078986196 MSE test 3.0839619027081677\n",
      "Epoch 86 / 200 loss: 127.47088980674744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train 2.6381636272929683 MSE test 3.0690478000402153\n",
      "Epoch 87 / 200 loss: 126.70005941390991\n",
      "MSE train 2.6233239792878487 MSE test 3.05440022622542\n",
      "Epoch 88 / 200 loss: 125.96895611286163\n",
      "MSE train 2.608837936670228 MSE test 3.0399516017520294\n",
      "Epoch 89 / 200 loss: 125.25940680503845\n",
      "MSE train 2.5944549122176386 MSE test 3.025760806645409\n",
      "Epoch 90 / 200 loss: 124.56600201129913\n",
      "MSE train 2.579676499130881 MSE test 3.0115063328656797\n",
      "Epoch 91 / 200 loss: 123.87492084503174\n",
      "MSE train 2.5645282613525793 MSE test 2.997226817896652\n",
      "Epoch 92 / 200 loss: 123.16407585144043\n",
      "MSE train 2.5494610748676387 MSE test 2.9831148156164433\n",
      "Epoch 93 / 200 loss: 122.43786382675171\n",
      "MSE train 2.534651545489577 MSE test 2.9692299045062596\n",
      "Epoch 94 / 200 loss: 121.71763062477112\n",
      "MSE train 2.5201742413120165 MSE test 2.9556650710153405\n",
      "Epoch 95 / 200 loss: 121.0106920003891\n",
      "MSE train 2.5058281535574887 MSE test 2.9422867971982583\n",
      "Epoch 96 / 200 loss: 120.31783390045166\n",
      "MSE train 2.4916525398096647 MSE test 2.929032674363171\n",
      "Epoch 97 / 200 loss: 119.63081276416779\n",
      "MSE train 2.478004732938074 MSE test 2.91614973130623\n",
      "Epoch 98 / 200 loss: 118.95310413837433\n",
      "MSE train 2.464841216712316 MSE test 2.903657958896828\n",
      "Epoch 99 / 200 loss: 118.3017041683197\n",
      "MSE train 2.4519116514012675 MSE test 2.8915154095344\n",
      "Epoch 100 / 200 loss: 117.6726154088974\n",
      "MSE train 2.4392741046321396 MSE test 2.879751728341962\n",
      "Epoch 101 / 200 loss: 117.05445313453674\n",
      "MSE train 2.427266644263626 MSE test 2.8683422154393696\n",
      "Epoch 102 / 200 loss: 116.45156610012054\n",
      "MSE train 2.4155969700205806 MSE test 2.85709845552099\n",
      "Epoch 103 / 200 loss: 115.87823843955994\n",
      "MSE train 2.4040250745207854 MSE test 2.8460031328170254\n",
      "Epoch 104 / 200 loss: 115.32100236415863\n",
      "MSE train 2.3924251628322337 MSE test 2.835033021316995\n",
      "Epoch 105 / 200 loss: 114.76806020736694\n",
      "MSE train 2.380897089937619 MSE test 2.824234578317415\n",
      "Epoch 106 / 200 loss: 114.21311044692993\n",
      "MSE train 2.3696995487635406 MSE test 2.8136824536616185\n",
      "Epoch 107 / 200 loss: 113.66216921806335\n",
      "MSE train 2.358753743783195 MSE test 2.8032268629540833\n",
      "Epoch 108 / 200 loss: 113.12671864032745\n",
      "MSE train 2.347902259818921 MSE test 2.7928432695414194\n",
      "Epoch 109 / 200 loss: 112.6031424999237\n",
      "MSE train 2.337048659196301 MSE test 2.7825409621172286\n",
      "Epoch 110 / 200 loss: 112.0836740732193\n",
      "MSE train 2.3261226445118255 MSE test 2.7722380310279284\n",
      "Epoch 111 / 200 loss: 111.56501078605652\n",
      "MSE train 2.315188705436437 MSE test 2.761899729846839\n",
      "Epoch 112 / 200 loss: 111.0428661108017\n",
      "MSE train 2.304356149262982 MSE test 2.7516878820985\n",
      "Epoch 113 / 200 loss: 110.52114617824554\n",
      "MSE train 2.293875254072455 MSE test 2.7416927043104558\n",
      "Epoch 114 / 200 loss: 110.0045393705368\n",
      "MSE train 2.2838279210382093 MSE test 2.732010321845667\n",
      "Epoch 115 / 200 loss: 109.50654172897339\n",
      "MSE train 2.2741434899530786 MSE test 2.7224813945989634\n",
      "Epoch 116 / 200 loss: 109.0275342464447\n",
      "MSE train 2.264635150266037 MSE test 2.713159546959905\n",
      "Epoch 117 / 200 loss: 108.56523084640503\n",
      "MSE train 2.2548599683937907 MSE test 2.7039196793830196\n",
      "Epoch 118 / 200 loss: 108.10948979854584\n",
      "MSE train 2.245027651380429 MSE test 2.6947481452475146\n",
      "Epoch 119 / 200 loss: 107.64059817790985\n",
      "MSE train 2.235527755179489 MSE test 2.68576968473697\n",
      "Epoch 120 / 200 loss: 107.1707444190979\n",
      "MSE train 2.226202461364246 MSE test 2.676983176781763\n",
      "Epoch 121 / 200 loss: 106.71578979492188\n",
      "MSE train 2.216949914273023 MSE test 2.668337852173476\n",
      "Epoch 122 / 200 loss: 106.26873707771301\n",
      "MSE train 2.2080458477755776 MSE test 2.6599107788126086\n",
      "Epoch 123 / 200 loss: 105.82667541503906\n",
      "MSE train 2.1995415775867717 MSE test 2.651811590850839\n",
      "Epoch 124 / 200 loss: 105.40246140956879\n",
      "MSE train 2.1911845184581336 MSE test 2.6439440569837607\n",
      "Epoch 125 / 200 loss: 104.99735176563263\n",
      "MSE train 2.1830060316566713 MSE test 2.636278886121678\n",
      "Epoch 126 / 200 loss: 104.59919738769531\n",
      "MSE train 2.1748795424781244 MSE test 2.628713951210089\n",
      "Epoch 127 / 200 loss: 104.2089866399765\n",
      "MSE train 2.1668477064535057 MSE test 2.621245752984482\n",
      "Epoch 128 / 200 loss: 103.82081937789917\n",
      "MSE train 2.158828508176924 MSE test 2.613953175949972\n",
      "Epoch 129 / 200 loss: 103.43620991706848\n",
      "MSE train 2.1509185318529935 MSE test 2.6068005935982477\n",
      "Epoch 130 / 200 loss: 103.05108428001404\n",
      "MSE train 2.143436487353899 MSE test 2.599836089573573\n",
      "Epoch 131 / 200 loss: 102.67634093761444\n",
      "MSE train 2.1362445964723085 MSE test 2.5930632203550648\n",
      "Epoch 132 / 200 loss: 102.32004129886627\n",
      "MSE train 2.129156602116864 MSE test 2.5864152583002897\n",
      "Epoch 133 / 200 loss: 101.97685611248016\n",
      "MSE train 2.121972114198964 MSE test 2.5798293068968605\n",
      "Epoch 134 / 200 loss: 101.63807594776154\n",
      "MSE train 2.1146724477978185 MSE test 2.5732650747794676\n",
      "Epoch 135 / 200 loss: 101.2942327260971\n",
      "MSE train 2.107512180382076 MSE test 2.5667852503851054\n",
      "Epoch 136 / 200 loss: 100.94514226913452\n",
      "MSE train 2.1005282904184184 MSE test 2.560436513368887\n",
      "Epoch 137 / 200 loss: 100.60339760780334\n",
      "MSE train 2.0935426588423103 MSE test 2.5541791409222823\n",
      "Epoch 138 / 200 loss: 100.26918053627014\n",
      "MSE train 2.086661146570003 MSE test 2.547993890456668\n",
      "Epoch 139 / 200 loss: 99.93587446212769\n",
      "MSE train 2.07974152856624 MSE test 2.541849795137357\n",
      "Epoch 140 / 200 loss: 99.60685276985168\n",
      "MSE train 2.072750667083647 MSE test 2.5357364369987763\n",
      "Epoch 141 / 200 loss: 99.27563142776489\n",
      "MSE train 2.0658960770491985 MSE test 2.529707246554837\n",
      "Epoch 142 / 200 loss: 98.94176435470581\n",
      "MSE train 2.059189022267641 MSE test 2.5237788916337984\n",
      "Epoch 143 / 200 loss: 98.61438047885895\n",
      "MSE train 2.0525689293409557 MSE test 2.5179678142101687\n",
      "Epoch 144 / 200 loss: 98.29343545436859\n",
      "MSE train 2.0460366293222667 MSE test 2.512264065861723\n",
      "Epoch 145 / 200 loss: 97.97642314434052\n",
      "MSE train 2.0397754776424035 MSE test 2.5067154654637234\n",
      "Epoch 146 / 200 loss: 97.6650161743164\n",
      "MSE train 2.0336905393161118 MSE test 2.501309462307112\n",
      "Epoch 147 / 200 loss: 97.36508584022522\n",
      "MSE train 2.027836925448845 MSE test 2.496059782582939\n",
      "Epoch 148 / 200 loss: 97.07420754432678\n",
      "MSE train 2.0221143501556553 MSE test 2.4909075816298976\n",
      "Epoch 149 / 200 loss: 96.7941290140152\n",
      "MSE train 2.0165149324231484 MSE test 2.4858299519803673\n",
      "Epoch 150 / 200 loss: 96.5201895236969\n",
      "MSE train 2.0110270094175373 MSE test 2.4808084125148775\n",
      "Epoch 151 / 200 loss: 96.25224101543427\n",
      "MSE train 2.0054580553425616 MSE test 2.475784406007226\n",
      "Epoch 152 / 200 loss: 95.9890593290329\n",
      "MSE train 1.9994694963777582 MSE test 2.4707110492937168\n",
      "Epoch 153 / 200 loss: 95.7205046415329\n",
      "MSE train 1.993281415080137 MSE test 2.465572291808175\n",
      "Epoch 154 / 200 loss: 95.43183135986328\n",
      "MSE train 1.9872890680681252 MSE test 2.460461339927026\n",
      "Epoch 155 / 200 loss: 95.13787937164307\n",
      "MSE train 1.98140679423601 MSE test 2.455412914282742\n",
      "Epoch 156 / 200 loss: 94.85073447227478\n",
      "MSE train 1.9758474788454135 MSE test 2.4505158932008757\n",
      "Epoch 157 / 200 loss: 94.5695024728775\n",
      "MSE train 1.970505315307297 MSE test 2.445708729971087\n",
      "Epoch 158 / 200 loss: 94.30339825153351\n",
      "MSE train 1.965249261279459 MSE test 2.4409458233039207\n",
      "Epoch 159 / 200 loss: 94.04780566692352\n",
      "MSE train 1.9600719063532914 MSE test 2.4362158144282797\n",
      "Epoch 160 / 200 loss: 93.79623675346375\n",
      "MSE train 1.9550364512851541 MSE test 2.4315341586638506\n",
      "Epoch 161 / 200 loss: 93.54835796356201\n",
      "MSE train 1.9501243847872127 MSE test 2.4269145228148417\n",
      "Epoch 162 / 200 loss: 93.30731391906738\n",
      "MSE train 1.945241553922926 MSE test 2.422322533783106\n",
      "Epoch 163 / 200 loss: 93.07213854789734\n",
      "MSE train 1.9402480638579942 MSE test 2.4177181793180407\n",
      "Epoch 164 / 200 loss: 92.83827078342438\n",
      "MSE train 1.9351062801432057 MSE test 2.4130985663951936\n",
      "Epoch 165 / 200 loss: 92.59885764122009\n",
      "MSE train 1.9299805751710988 MSE test 2.4085305217730832\n",
      "Epoch 166 / 200 loss: 92.3523633480072\n",
      "MSE train 1.9248641896678693 MSE test 2.404052083070918\n",
      "Epoch 167 / 200 loss: 92.10668587684631\n",
      "MSE train 1.919658918142818 MSE test 2.3995806431626754\n",
      "Epoch 168 / 200 loss: 91.86104369163513\n",
      "MSE train 1.9145071812263177 MSE test 2.3950869404219626\n",
      "Epoch 169 / 200 loss: 91.61159539222717\n",
      "MSE train 1.9093431123763793 MSE test 2.3905490259479656\n",
      "Epoch 170 / 200 loss: 91.36477327346802\n",
      "MSE train 1.9043807832534254 MSE test 2.386126472448959\n",
      "Epoch 171 / 200 loss: 91.11741304397583\n",
      "MSE train 1.8995799560760391 MSE test 2.381776990776672\n",
      "Epoch 172 / 200 loss: 90.88006579875946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train 1.8948630031665794 MSE test 2.3774756663464682\n",
      "Epoch 173 / 200 loss: 90.65025770664215\n",
      "MSE train 1.8901964458979623 MSE test 2.3731846931636627\n",
      "Epoch 174 / 200 loss: 90.42403721809387\n",
      "MSE train 1.885539147168526 MSE test 2.3688928384001353\n",
      "Epoch 175 / 200 loss: 90.20000171661377\n",
      "MSE train 1.8808472959333904 MSE test 2.364582982481707\n",
      "Epoch 176 / 200 loss: 89.9766274690628\n",
      "MSE train 1.8761525408280908 MSE test 2.3602799422045617\n",
      "Epoch 177 / 200 loss: 89.75187695026398\n",
      "MSE train 1.8714094650357418 MSE test 2.356025854620243\n",
      "Epoch 178 / 200 loss: 89.52642953395844\n",
      "MSE train 1.8666026543385732 MSE test 2.351771694841121\n",
      "Epoch 179 / 200 loss: 89.29800391197205\n",
      "MSE train 1.8618545174061423 MSE test 2.347574119248909\n",
      "Epoch 180 / 200 loss: 89.06757509708405\n",
      "MSE train 1.8570167769165877 MSE test 2.343409053591541\n",
      "Epoch 181 / 200 loss: 88.84009897708893\n",
      "MSE train 1.8520951184636727 MSE test 2.3392625073361226\n",
      "Epoch 182 / 200 loss: 88.60793697834015\n",
      "MSE train 1.8470857857233438 MSE test 2.3351570006149194\n",
      "Epoch 183 / 200 loss: 88.37175452709198\n",
      "MSE train 1.8422909887899395 MSE test 2.331149507500339\n",
      "Epoch 184 / 200 loss: 88.13184654712677\n",
      "MSE train 1.8378404867439193 MSE test 2.327270375471717\n",
      "Epoch 185 / 200 loss: 87.90259611606598\n",
      "MSE train 1.8335826304608362 MSE test 2.3235024371660433\n",
      "Epoch 186 / 200 loss: 87.6893607378006\n",
      "MSE train 1.8294932469205252 MSE test 2.3198286011999296\n",
      "Epoch 187 / 200 loss: 87.48552405834198\n",
      "MSE train 1.825449231064363 MSE test 2.316231357366939\n",
      "Epoch 188 / 200 loss: 87.28960680961609\n",
      "MSE train 1.821508295664641 MSE test 2.3126084386792254\n",
      "Epoch 189 / 200 loss: 87.09460258483887\n",
      "MSE train 1.8179627288746656 MSE test 2.3091031771130512\n",
      "Epoch 190 / 200 loss: 86.9085156917572\n",
      "MSE train 1.8145731192014347 MSE test 2.30564781838761\n",
      "Epoch 191 / 200 loss: 86.73949360847473\n",
      "MSE train 1.8112181047692577 MSE test 2.3022091647186893\n",
      "Epoch 192 / 200 loss: 86.57771074771881\n",
      "MSE train 1.8077815068013168 MSE test 2.298725736467892\n",
      "Epoch 193 / 200 loss: 86.41716694831848\n",
      "MSE train 1.8042311575759264 MSE test 2.2954142462072484\n",
      "Epoch 194 / 200 loss: 86.25262665748596\n",
      "MSE train 1.8006356908710728 MSE test 2.2921362807181707\n",
      "Epoch 195 / 200 loss: 86.08263421058655\n",
      "MSE train 1.7971554081084042 MSE test 2.2888974631901347\n",
      "Epoch 196 / 200 loss: 85.91052031517029\n",
      "MSE train 1.7936828696570373 MSE test 2.285702618328073\n",
      "Epoch 197 / 200 loss: 85.74375784397125\n",
      "MSE train 1.790206468914315 MSE test 2.2825792854686746\n",
      "Epoch 198 / 200 loss: 85.57759082317352\n",
      "MSE train 1.7866728496959632 MSE test 2.2794620573063615\n",
      "Epoch 199 / 200 loss: 85.41103780269623\n"
     ]
    }
   ],
   "source": [
    "# running the model for a 200 epochs taking 100 users in batches\n",
    "# total improvement is printed out after each epoch\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0    # initializing error as 0\n",
    "    \n",
    "    for i in range(int(tot_users/batch_size)):\n",
    "        epoch_x = X_train[ i*batch_size : (i+1)*batch_size ]\n",
    "        _, c = sess.run([optimizer, meansq],\\\n",
    "               feed_dict={input_layer: epoch_x, \\\n",
    "               output_true: epoch_x})\n",
    "        epoch_loss += c\n",
    "        \n",
    "    output_train = sess.run(output_layer, feed_dict={input_layer:X_train})\n",
    "    output_test = sess.run(output_layer, feed_dict={input_layer:X_test})\n",
    "        \n",
    "    print('MSE train', MSE(output_train, X_train),'MSE test', MSE(output_test, X_test))      \n",
    "    print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a user\n",
    "sample_user = X_test.iloc[99,:]\n",
    "#get the predicted ratings\n",
    "sample_user_pred = sess.run(output_layer, feed_dict={input_layer:[sample_user]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3706,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "5       0.0\n",
       "6       0.0\n",
       "7       0.0\n",
       "8       0.0\n",
       "9       0.0\n",
       "10      0.0\n",
       "11      3.0\n",
       "12      0.0\n",
       "13      0.0\n",
       "14      0.0\n",
       "15      0.0\n",
       "16      0.0\n",
       "17      0.0\n",
       "18      0.0\n",
       "19      0.0\n",
       "20      0.0\n",
       "21      0.0\n",
       "22      0.0\n",
       "23      0.0\n",
       "24      0.0\n",
       "25      0.0\n",
       "26      0.0\n",
       "27      0.0\n",
       "28      0.0\n",
       "29      0.0\n",
       "30      0.0\n",
       "       ... \n",
       "3923    0.0\n",
       "3924    0.0\n",
       "3925    0.0\n",
       "3926    0.0\n",
       "3927    0.0\n",
       "3928    0.0\n",
       "3929    0.0\n",
       "3930    0.0\n",
       "3931    0.0\n",
       "3932    0.0\n",
       "3933    0.0\n",
       "3934    0.0\n",
       "3935    0.0\n",
       "3936    0.0\n",
       "3937    0.0\n",
       "3938    0.0\n",
       "3939    0.0\n",
       "3940    0.0\n",
       "3941    0.0\n",
       "3942    0.0\n",
       "3943    0.0\n",
       "3944    0.0\n",
       "3945    0.0\n",
       "3946    0.0\n",
       "3947    0.0\n",
       "3948    0.0\n",
       "3949    0.0\n",
       "3950    0.0\n",
       "3951    0.0\n",
       "3952    0.0\n",
       "Name: 4792, Length: 3706, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(sample_user.shape)\n",
    "sample_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3706)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.7229145 ,  0.9290279 ,  0.06855589, ...,  1.3700454 ,\n",
       "        -0.06047785, -0.4500605 ]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(sample_user_pred.shape)\n",
    "sample_user_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
